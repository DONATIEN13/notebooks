{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8781051a-a327-4ee4-89f3-a310f0dcf874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5f09e69-a981-4455-ac48-0b8dfeebcdc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résumé: Les séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Le choix automatique de ce paramètre de régularisation est un problème statistique majeur.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considér\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Étape 1 : Charger le modèle et le tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Étape 2 : Préparer les documents source\n",
    "documents = [\"Il existe en effet une infinité d'hyperplans séparateurs, dont les performances en apprentissage sont identiques (le risque empirique est le même), mais dont les performances en généralisation peuvent être très différentes. L'idée des variables ressorts, qui permet de résoudre certaines limitations pratiques importantes, ne sera introduite qu'en 1995. Ensuite, les caractéristiques des nouvelles données peuvent être utilisées pour prédire le groupe auquel un nouvel enregistrement doit appartenir.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Le choix automatique de ce paramètre de régularisation est un problème statistique majeur.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Le choix automatique de ce paramètre de régularisation est un problème statistique majeur.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Ils ont rapidement été adoptés pour leur capacité à travailler avec des données de grandes dimensions, le faible nombre d'hyperparamètres, leurs garanties théoriques, et leurs bons résultats en pratique. Les fondations théoriques des SVM ont été explorées par Vapnik et ses collègues dans les années 70 avec le développement de la théorie de Vapnik-Tchervonenkis, et par Valiant et la théorie de l'apprentissage PAC[4].nnIl existe des raisons théoriques à ce choix.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Ils ont rapidement été adoptés pour leur capacité à travailler avec des données de grandes dimensions, le faible nombre d'hyperparamètres, leurs garanties théoriques, et leurs bons résultats en pratique. Les fondations théoriques des SVM ont été explorées par Vapnik et ses collègues dans les années 70 avec le développement de la théorie de Vapnik-Tchervonenkis, et par Valiant et la théorie de l'apprentissage PAC[4].nnIl existe des raisons théoriques à ce choix.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Ils ont rapidement été adoptés pour leur capacité à travailler avec des données de grandes dimensions, le faible nombre d'hyperparamètres, leurs garanties théoriques, et leurs bons résultats en pratique. Les fondations théoriques des SVM ont été explorées par Vapnik et ses collègues dans les années 70 avec le développement de la théorie de Vapnik-Tchervonenkis, et par Valiant et la théorie de l'apprentissage PAC[4].nnIl existe des raisons théoriques à ce choix.nnLes séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Ils ont rapidement été adoptés pour leur capacité à travailler avec des données de grandes dimensions, le faible nombre d'hyperparamètres, leurs garanties théoriques, et leurs bons résultats en pratique. Les fondations théoriques des SVM ont été explorées par Vapnik et ses collègues dans les années 70 avec le développement de la théorie de Vapnik-Tchervonenkis, et par Valiant et la théorie de l'apprentissage PAC[4].nnIl existe des raisons théoriques à ce choix. Les fondations théoriques des SVM ont été explorées par Vapnik et ses collègues dans les années 70 avec le développement de la théorie de Vapnik-Tchervonenkis, et par Valiant et la théorie de l'apprentissage PAC[4]\"]\n",
    "\n",
    "# Étape 3 : Prétraitement et génération du résumé\n",
    "inputs = tokenizer.prepare_seq2seq_batch(documents, truncation=True, padding=\"max_length\")\n",
    "summary_ids = model.generate(inputs.input_ids, num_beams=6, max_length=300, early_stopping=True)\n",
    "summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "# Afficher les résumés générés\n",
    "for i, summary in enumerate(summaries):\n",
    "    print(f\"Résumé: {summary}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd0e44a3-d4d7-4157-9f71-2581ce91eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.23931623931623933, 'rouge2': 0.0, 'rougeL': 0.13675213675213674, 'rougeLsum': 0.13675213675213674}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"Les séparateurs à vaste marge ont été développés dans les années 1990 à partir des considérations théoriques de Vladimir Vapnik sur le développement d'une théorie statistique de l'apprentissage : la théorie de Vapnik-Tchervonenkis. Le choix automatique de ce paramètre de régularisation est un problème statistique majeur.\"]\n",
    "\n",
    "references = [\"Les SVM sont utilisés principalement pour la classification binaire, mais ils peuvent également être étendus à la classification multi-classes. Les données d'entraînement sont représentées comme des vecteurs dans un espace de caractéristiques de dimension finie. Chaque vecteur représente une instance de données, et chaque dimension représente une caractéristique.\"]\n",
    "\n",
    "rouge_results = rouge.compute(predictions = predictions, references = references)\n",
    "print(rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecac6350-7e9f-4e6a-9f58-3d2af581866a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0, 'precisions': [0.22916666666666666, 0.0], 'brevity_penalty': 0.9010751057212905, 'length_ratio': 0.9056603773584906, 'translation_length': 48, 'reference_length': 53}\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate.load('bleu')\n",
    "bleu_results = bleu.compute(predictions = predictions, references = references,  max_order = 2)\n",
    "print(bleu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d1d3b-2c7e-442c-9d3c-e01469207470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
